{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7cbe43-703a-4be0-98fe-92bb5b8a3c01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vuong/anaconda3/envs/torchtext_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/vuong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from underthesea import text_normalize\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from classes import SentimentDataset\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "from stopwordsiso import stopwords\n",
    "import string\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736280a4-43d2-44e5-8c4f-b6fc3fa77c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vuong/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /home/vuong/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.data.find('tokenizers/punkt')  # Kiểm tra xem đã có chưa\n",
    "nltk.download('punkt', force=True)\n",
    "nltk.download('punkt_tab')\n",
    "vietnamese_stopwords = stopwords(['vi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bb9765-ac0a-4436-aac6-79864567dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('sentiment_data.csv').dropna()\n",
    "# data = data.rename(columns={'sentence': 'text', 'sentiment': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b150689b-8f21-4407-af92-70e73bb795f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m test_df \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# # Concatenate all DataFrames into one (so data is more objective and balance)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mtrain_df\u001b[49m, val_df, test_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# # Randomly mix/shuffle the DataFrame\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# data = data.sample(frac=1).reset_index(drop=True)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m data\u001b[38;5;241m.\u001b[39mrename(columns \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"uit-nlp/vietnamese_students_feedback\") # https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8573337\n",
    "\n",
    "train_df = dataset['train'].to_pandas()\n",
    "val_df = dataset['validation'].to_pandas()\n",
    "test_df = dataset['test'].to_pandas()\n",
    "\n",
    "# # Concatenate all DataFrames into one (so data is more objective and balance)\n",
    "data = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "# # Randomly mix/shuffle the DataFrame\n",
    "# data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.rename(columns = {'sentence': 'text', 'sentiment': 'label'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca47019-cc8f-4d71-ac49-5c79498e30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd22a2-26cc-43b4-82bf-51092f456628",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a073f-34af-491a-8d05-6370d0caf4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee00c2-35db-475d-9575-f638b610ff72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duplicates = data[data.duplicated(subset='text', keep=False)]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7c645-2624-48f2-984c-971b5c9aa1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=['text'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175fee4-7b1d-418f-b110-dbbbe69876e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ced670-cbbb-435a-b686-6a12e9e99004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "x = sb.countplot(x='label',data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1e81a-2851-4d63-88ff-aa79b1297975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Làm sạch dữ liệu trong DataFrame gốc\n",
    "# data[\"text\"] = data[\"text\"].str.strip().str.replace(\"\\n\", \" \")\n",
    "\n",
    "# # Tiền xử lý: chuyển thường, loại bỏ ký tự đặc biệt, khoảng trắng dư thừa\n",
    "# texts = [re.sub(r\"[^\\w\\s]\", \"\", t.lower()) for t in data[\"text\"]]\n",
    "# texts = [' '.join(t.split()) for t in texts]\n",
    "\n",
    "# # Tokenization thủ công (nếu muốn giữ dưới dạng list các từ)\n",
    "# tokens = [t.split() for t in texts]\n",
    "\n",
    "# # Ghép lại thành văn bản nếu cần đầu ra là string\n",
    "# texts = [' '.join(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c5421-a3c0-4f90-a77f-1103c1dfd06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"\n",
    "                u\"\\U0001F300-\\U0001F5FF\" \n",
    "                u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "    \"]+\", flags=re.UNICODE) # Unicode emojis. \n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    \n",
    "    text = re.sub(emoji_pattern, \" \", text) # remove emojis\n",
    "    \n",
    "    text = re.sub(r'([a-z]+?)\\1+',r'\\1', text) # reduce repeated character (e.g. 'aaabbb' -> 'ab')\n",
    "    \n",
    "    # Ensure space before and after any punctuation mark\n",
    "    text = re.sub(r\"(\\w)\\s*([\" + string.punctuation + \"])\\s*(\\w)\", r\"\\1 \\2 \\3\", text)\n",
    "    text = re.sub(r\"(\\w)\\s*([\" + string.punctuation + \"])\", r\"\\1 \\2\", text)\n",
    "    \n",
    "    text = re.sub(f\"([{string.punctuation}])([{string.punctuation}])+\",r\"\\1\", text) # reduce consecutive punctuation\n",
    "    \n",
    "    # Remove any leading or trailing spaces, or leading or trailing punctuation marks from the text\n",
    "    text = text.strip()\n",
    "    while text.endswith(tuple(string.punctuation+string.whitespace)):\n",
    "        text = text[:-1]\n",
    "    while text.startswith(tuple(string.punctuation+string.whitespace)):\n",
    "        text = text[1:]\n",
    "        \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuation\n",
    "        \n",
    "    text = re.sub(r\"\\s+\", \" \", text) # reduce multiple spaces\n",
    "    \n",
    "    text = text_normalize(text) # make sure punctunation is in the right letter (Vietnamese case)\n",
    "    text = word_tokenize(text, format=\"text\") # tokenize the cleaned text\n",
    "    # text = unidecode(text) # remove accent marks from sentences (no significant difference when accent marks is removed or kept)\n",
    "    \n",
    "    '''\n",
    "    Proper tokenization allows models or algorithms to understand the semantics of the text better. \n",
    "    For instance, `\"đi chơi\" (go play)` should be treated as one unit \n",
    "    instead of two separate words \"đi\" (go) and \"chơi\" (play) for proper semantic understanding.\n",
    "    '''\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb209de5-c977-4bc3-bb3a-e1c3c282e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['corpus'] = data['text'].map(lambda text: clean_text(text))                                                     \n",
    "# Final check\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e1da6-dcd3-45c1-be34-0d9b83cef21b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# texts = data['corpus']\n",
    "# len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea01e6-6911-4ef2-b9b5-08f67b723ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# labels = data['label'].map({'negative': 0, 'positive': 1, 'neutral': 2}).tolist()\n",
    "# labels = data['label']\n",
    "labels = data['label'].tolist()\n",
    "texts = data['corpus']\n",
    "# dataset = SentimentDataset(texts, labels)\n",
    "\n",
    "# tokenized_texts = [word_tokenize(t) for t in texts]\n",
    "\n",
    "tokenized_texts = [\n",
    "    [word for word in word_tokenize(t) if word not in vietnamese_stopwords]\n",
    "    for t in texts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368e52f-b8f7-4ebe-8535-02ec9952a293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(texts), len(labels))  # Cần phải bằng nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce2500f-55fd-4d11-bbee-cc013c52d099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "lengths = [len(seq) for seq in tokenized_texts]\n",
    "\n",
    "print(\"Max length:\", max(lengths))\n",
    "print(\"Mean length:\", np.mean(lengths))\n",
    "print(\"Median length:\", np.median(lengths))\n",
    "print(\"90th percentile:\", np.percentile(lengths, 90))\n",
    "print(\"95th percentile:\", np.percentile(lengths, 95))\n",
    "print(\"99th percentile:\", np.percentile(lengths, 99))\n",
    "\n",
    "sns.histplot(lengths, bins=50, kde=True)\n",
    "plt.title(\"Phân phối độ dài câu (sau token hóa)\")\n",
    "plt.xlabel(\"Số token\")\n",
    "plt.ylabel(\"Số lượng văn bản\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce9189-3b65-4db9-9c8c-f600f9be7ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "all_words = [w for txt in tokenized_texts for w in txt]\n",
    "most_common = Counter(all_words).most_common(4998)\n",
    "\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "for i, (w, _) in enumerate(most_common, 2):\n",
    "    vocab[w] = i\n",
    "\n",
    "def to_indices(tokens, max_len):\n",
    "    idxs = [vocab.get(t, 1) for t in tokens][:max_len]\n",
    "    return idxs + [0] * (max_len - len(idxs))\n",
    "\n",
    "max_len_text = 20\n",
    "text_indices = [to_indices(t, max_len_text) for t in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f40e90-5011-4a0b-91d0-d9ac895cfb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(text_indices, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d078c23-2040-4b1d-a2ee-67312e654545",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(train_texts, train_labels)\n",
    "test_dataset = SentimentDataset(test_texts, test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07daa7-71bc-4dfc-b20e-75a9f58a6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = {\n",
    "    'train_loader': train_loader,\n",
    "    'test_loader': test_loader,\n",
    "    'batch_size': 32,\n",
    "    'vocab': vocab,\n",
    "    'vocab_size': len(vocab)\n",
    "}\n",
    "save_path = 'sentiment_data_loader.pth'\n",
    "torch.save(data_to_save, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be1bb89-ab49-4fcd-b4f6-17d7383f600f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
