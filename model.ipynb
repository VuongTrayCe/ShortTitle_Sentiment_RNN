{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac00ca05-83d1-431e-bfde-714ecfcdcb33",
   "metadata": {},
   "source": [
    "# import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6695f9e-ba33-47ad-86d4-88b7737b8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from classes import SentimentDataset\n",
    "import torch.optim as optim\n",
    "load_path = 'sentiment_data_loader.pth' # Đảm bảo đường dẫn này chính xác\n",
    "loaded_data = torch.load(load_path)\n",
    "batch_size = loaded_data.get('batch_size', 32)\n",
    "train_loader = loaded_data.get('train_loader')\n",
    "test_loader = loaded_data.get('test_loader')\n",
    "vocab = loaded_data.get('vocab')\n",
    "vocab_size = loaded_data.get('vocab_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c21ec55-0092-4d23-809f-d50c31c004a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2307"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452e7e2-fe1f-4c72-9fc1-6b8b1b762b35",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "067c4337-bca1-4970-aff2-f3973b39b50b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Phul luc B: model.py\n",
    "import torch.nn as nn\n",
    "import torchtext.vocab as tvocab\n",
    "import numpy as np\n",
    "\n",
    "# --- Helper function to load GloVe embeddings ---\n",
    "def load_glove_embeddings(glove_path, vocab, embedding_dim):\n",
    "    \"\"\"\n",
    "    Loads GloVe embeddings for words found in the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        glove_path (str): Name of the GloVe vectors (e.g., 'glove.6B.100d').\n",
    "                          Make sure you have downloaded these or torchtext can download them.\n",
    "        vocab (dict): The vocabulary mapping words to indices.\n",
    "        embedding_dim (int): The dimension of the GloVe embeddings.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The embedding matrix.\n",
    "    \"\"\"\n",
    "    print(f\"Loading GloVe vectors: {glove_path}...\")\n",
    "    # Tải GloVe vectors sử dụng torchtext\n",
    "    # Lần đầu chạy có thể mất thời gian để tải file GloVe\n",
    "    try:\n",
    "        glove = tvocab.GloVe(name=glove_path.split('.')[1], # e.g., '6B'\n",
    "                             dim=embedding_dim,            # e.g., 100\n",
    "                             cache='.vector_cache')        # Thư mục lưu cache\n",
    "        print(\"GloVe vectors loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GloVe vectors: {e}\")\n",
    "        print(\"Please ensure the GloVe files are available or can be downloaded.\")\n",
    "        print(\"You might need to install torchtext: pip install torchtext\")\n",
    "        # Hoặc tải thủ công từ: https://nlp.stanford.edu/projects/glove/\n",
    "        # và giải nén vào thư mục .vector_cache\n",
    "        raise e # Dừng chương trình nếu không tải được GloVe\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    # Khởi tạo ma trận embedding với giá trị ngẫu nhiên nhỏ\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (vocab_size, embedding_dim))\n",
    "    embeddings[vocab['<PAD>']] = np.zeros(embedding_dim) # Vector 0 cho PAD\n",
    "\n",
    "    # Điền vào ma trận embedding bằng vector GloVe nếu từ có trong GloVe\n",
    "    loaded_count = 0\n",
    "    for word, idx in vocab.items():\n",
    "        if word in glove.stoi: # stoi: string-to-index mapping trong GloVe object\n",
    "\n",
    "            embeddings[idx] = glove.vectors[glove.stoi[word]].numpy()\n",
    "            loaded_count += 1\n",
    "        # else: để lại giá trị khởi tạo ngẫu nhiên (hoặc có thể gán vector <UNK> nếu muốn)\n",
    "\n",
    "    print(f\"Loaded {loaded_count} vectors from GloVe out of {vocab_size} vocab size.\")\n",
    "    return torch.tensor(embeddings, dtype=torch.float)\n",
    "# --------------------------------------------------\n",
    "#pretrained_embeddings = load_glove_embeddings('glove.6B.100d',vocab,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3cbcd36f-074f-49b0-83d9-2b726f606ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,vocab, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 pad_idx, pretrained=False, glove_path='glove.6B.100d'): # Thêm vocab và glove_path\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = pad_idx # Lấy index của PAD token\n",
    "\n",
    "        # --- Khởi tạo embedding layer ---\n",
    "        if pretrained:\n",
    "            print(\"Using pre-trained GloVe embeddings.\")\n",
    "            # Tải trọng số GloVe\n",
    "            pretrained_embeddings = load_glove_embeddings(glove_path, vocab, embedding_dim)\n",
    "            # Tạo lớp Embedding từ trọng số đã tải\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                pretrained_embeddings,\n",
    "                freeze=False, # Cho phép fine-tuning embedding nếu muốn (False)\n",
    "                padding_idx=self.padding_idx\n",
    "            )\n",
    "        else:\n",
    "            print(\"Training embeddings from scratch.\")\n",
    "            # Khởi tạo embedding ngẫu nhiên\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings=vocab_size,\n",
    "                embedding_dim=embedding_dim,\n",
    "                padding_idx=self.padding_idx\n",
    "            )\n",
    "\n",
    "        # --- Khởi tạo khối RNN layer ---\n",
    "        # [Sinh viên bổ sung: dùng nn.RNN với batch_first=True]\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1, # Giữ đơn giản với 1 lớp RNN\n",
    "            batch_first=True, # Quan trọng: input/output có dạng (batch, seq, feature)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # --- Khởi tạo tầng Dense để dự đoán 3 nhãn ---\n",
    "        # [Sinh viên bổ sung: dùng nn.Linear, nhận hidden state từ RNN]\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=hidden_dim, # Input là hidden state cuối cùng của RNN\n",
    "            out_features=output_dim # Output là số lớp cảm xúc (3)\n",
    "        )\n",
    "        # --- Hết phần bổ sung Dense ---\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: (batch_size, seq_len)\n",
    "\n",
    "        # --- Chuyển text thành embedding ---\n",
    "        # [Sinh viên bổ sung]\n",
    "        # embedded shape: (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(text)\n",
    "        # --- Hết phần bổ sung embedding forward ---\n",
    "\n",
    "        # --- Đưa qua khối RNN để lẩy hidden state cuối ---\n",
    "        # [Sinh viên bổ sung]\n",
    "        # output shape: (batch_size, seq_len, hidden_dim)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_dim) -> (1, batch_size, hidden_dim)\n",
    "        rnn_output, hidden = self.rnn(embedded)\n",
    "        #_, (hidden, _) = self.rnn(embedded)\n",
    "        # Lấy hidden state cuối cùng của lớp RNN duy nhất\n",
    "        # hidden.squeeze(0) loại bỏ chiều num_layers (vì = 1)\n",
    "        # last_hidden shape: (batch_size, hidden_dim)\n",
    "        #last_hidden = hidden.squeeze(0)\n",
    "        last_hidden = hidden[-1] \n",
    "        # --- Hết phần bổ sung RNN forward ---\n",
    "\n",
    "        # --- Đưa hidden state qua tầng Dense để dự đoán 3 nhãn ---\n",
    "        # [Sinh viên bổ sung]\n",
    "        # predictions shape: (batch_size, output_dim)\n",
    "        last_hidden = self.dropout(last_hidden)  # <-- thêm dòng này\n",
    "\n",
    "        predictions = self.fc(last_hidden)\n",
    "        # --- Hết phần bổ sung Dense forward ---\n",
    "\n",
    "        # [Sinh viên bổ sung: trả về kết quả dự đoán]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1fd07bc6-295e-482f-9ff9-64add9c46487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 pad_idx, pretrained=False, glove_path='glove.6B.100d'): # Thêm vocab và glove_path\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = pad_idx # Lấy index của PAD token\n",
    "\n",
    "        # --- Khởi tạo embedding layer ---\n",
    "        if pretrained:\n",
    "            print(\"Using pre-trained GloVe embeddings.\")\n",
    "            # Tải trọng số GloVe\n",
    "            pretrained_embeddings = load_glove_embeddings(glove_path, vocab, embedding_dim)\n",
    "            # Tạo lớp Embedding từ trọng số đã tải\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                pretrained_embeddings,\n",
    "                freeze=False, # Cho phép fine-tuning embedding nếu muốn (False)\n",
    "                padding_idx=self.padding_idx\n",
    "            )\n",
    "        else:\n",
    "            print(\"Training embeddings from scratch.\")\n",
    "            # Khởi tạo embedding ngẫu nhiên\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings=vocab_size,\n",
    "                embedding_dim=embedding_dim,\n",
    "                padding_idx=self.padding_idx\n",
    "            )\n",
    "\n",
    "        # --- Khởi tạo khối LSTM layer ---\n",
    "        # [Sinh viên bổ sung: dùng nn.LSTM với batch_first=True]\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1, # Giữ đơn giản với 1 lớp LSTM\n",
    "            batch_first=True, # Quan trọng: input/output có dạng (batch, seq, feature)\n",
    "        )\n",
    "\n",
    "        # --- Khởi tạo tầng Dense để dự đoán 3 nhãn ---\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=hidden_dim, # Input là hidden state cuối cùng của LSTM\n",
    "            out_features=output_dim # Output là số lớp cảm xúc (3)\n",
    "        )\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: (batch_size, seq_len)\n",
    "\n",
    "        # --- Chuyển text thành embedding ---\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # --- Đưa qua khối LSTM để lấy hidden state cuối ---\n",
    "        # output shape: (batch_size, seq_len, hidden_dim)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_dim) -> (1, batch_size, hidden_dim)\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "        # hidden có shape (num_layers, batch_size, hidden_dim), ta chỉ cần hidden[-1]\n",
    "        last_hidden = hidden[-1]  # LSTM output the last hidden state of the last layer\n",
    "\n",
    "        # --- Đưa hidden state qua tầng Dense để dự đoán 3 nhãn ---\n",
    "        predictions = self.fc(last_hidden)\n",
    "\n",
    "        # Trả về kết quả dự đoán\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823fda6-797f-4d36-9f6b-cbf0c5e863b7",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e422c91b-cad7-4c51-8f23-87dd5b8f004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "embedding_dim = 100 # Kích thước vector embedding\n",
    "hidden_dim = 128    # Kích thước lớp ẩn RNN\n",
    "output_dim = 3\n",
    "pad_idx = vocab[\"<PAD>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e090296f-2a80-4957-a8fc-7761b1c9cc62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training embeddings from scratch.\n"
     ]
    }
   ],
   "source": [
    "# # Giả sử vocab và vocab_size đã được import từ data.py\n",
    "model_glove= RNNModel(vocab,vocab_size, embedding_dim, hidden_dim, output_dim,pad_idx, pretrained=False,glove_path='glove.6B.100d')\n",
    "# # model_test_glove = RNNModel(vocab_size, embedding_dim_test, hidden_dim_test, output_dim_test, vocab, pretrained=True)\n",
    "# # print(model_test_scratch)\n",
    "# # print(model_test_glove)\n",
    "# model_glove= LSTMModel(vocab,vocab_size, embedding_dim, hidden_dim, output_dim,pad_idx, pretrained=False,glove_path='glove.6B.100d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f4f4a89-5e11-4313-a1d2-b05df6f0793d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "42977863-5f17-45f3-a6a9-e5a7e981b96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch [1/100], Train Loss: 0.8429, Train Acc: 0.4984, Val Loss: 0.8312, Val Acc: 0.5101\n",
      "Epoch [10/100], Train Loss: 0.7856, Train Acc: 0.6396, Val Loss: 0.9648, Val Acc: 0.5372\n",
      "Epoch [20/100], Train Loss: 0.8291, Train Acc: 0.6371, Val Loss: 0.8717, Val Acc: 0.6737\n",
      "Epoch [30/100], Train Loss: 0.8447, Train Acc: 0.6202, Val Loss: 0.8356, Val Acc: 0.6264\n",
      "Epoch [40/100], Train Loss: 0.8370, Train Acc: 0.6708, Val Loss: 1.0908, Val Acc: 0.6474\n",
      "Epoch [50/100], Train Loss: 2.3211, Train Acc: 0.5119, Val Loss: 1.0768, Val Acc: 0.5871\n",
      "Epoch [60/100], Train Loss: 0.9665, Train Acc: 0.4857, Val Loss: 0.9130, Val Acc: 0.4803\n",
      "Epoch [70/100], Train Loss: 0.9593, Train Acc: 0.4939, Val Loss: 0.9723, Val Acc: 0.4794\n",
      "Epoch [80/100], Train Loss: 0.9715, Train Acc: 0.4866, Val Loss: 1.2365, Val Acc: 0.5066\n",
      "Epoch [90/100], Train Loss: 0.9685, Train Acc: 0.4914, Val Loss: 1.0178, Val Acc: 0.4899\n",
      "Epoch [100/100], Train Loss: 1.0337, Train Acc: 0.4883, Val Loss: 1.0402, Val Acc: 0.4961\n",
      "--- Training Finished ---\n",
      "Total Training Time: 218.72 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# --- Xác định thiết bị (CPU hoặc GPU nếu có) ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "learning_rate = 0.1 # Tốc độ học cho SGD\n",
    "num_epochs = 100\n",
    "# Khởi tạo mô hình\n",
    "pad_idx = vocab['<PAD>']\n",
    "# model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx)\n",
    "#model = model.to(device) # Chuyển model lên device\n",
    "model = model_glove.to(device)\n",
    "# --- Bước 3: Định nghĩa Loss và Optimizer ---\n",
    "criterion = nn.CrossEntropyLoss() # Phù hợp cho bài toán phân loại đa lớp\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) # Sử dụng SGD theo yêu cầu\n",
    "\n",
    "# --- Bước 4: Huấn luyện mô hình ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Chuyển sang chế độ huấn luyện\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "\n",
    "    for batch_sequences, batch_labels in train_loader:\n",
    "        # Chuyển dữ liệu batch lên device\n",
    "        batch_sequences = batch_sequences.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        # 1. Xóa gradients cũ\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2. Forward pass\n",
    "        predictions = model(batch_sequences)\n",
    "\n",
    "\n",
    "        # 3. Tính loss\n",
    "        loss = criterion(predictions, batch_labels)\n",
    "        \n",
    "\n",
    "        # 4. Backward pass (tính gradient)\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Cập nhật trọng số\n",
    "        optimizer.step()\n",
    "\n",
    "        # Tính toán thống kê cho epoch\n",
    "        epoch_loss += loss.item()\n",
    "        preds_class = torch.argmax(predictions, dim=1)\n",
    "        epoch_correct += (preds_class == batch_labels).sum().item()\n",
    "        epoch_total += batch_labels.size(0)\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = epoch_correct / epoch_total\n",
    "     # --- Validation sau mỗi epoch ---\n",
    "    model.eval()  # Chuyển sang chế độ đánh giá (tắt dropout/batchnorm nếu có)\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  # Tắt tính toán gradient để tiết kiệm bộ nhớ\n",
    "        for val_sequences, val_labels in test_loader:\n",
    "            val_sequences = val_sequences.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "\n",
    "            val_predictions = model(val_sequences)\n",
    "            loss = criterion(val_predictions, val_labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_class = torch.argmax(val_predictions, dim=1)\n",
    "            val_correct += (preds_class == val_labels).sum().item()\n",
    "            val_total += val_labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    # In kết quả sau mỗi vài epoch để theo dõi\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {avg_epoch_loss:.4f}, Train Acc: {accuracy:.4f}, '\n",
    "              f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "end_time = time.time()\n",
    "print(f\"--- Training Finished ---\")\n",
    "print(f\"Total Training Time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
